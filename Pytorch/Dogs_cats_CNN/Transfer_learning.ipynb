{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ml_kernel",
   "display_name": "ML_kernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from statistics import mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(sys.argv[1]))\n",
    "TRANSFORM = transforms.Compose([transforms.Resize(128), transforms.CenterCrop(127), transforms.ToTensor()])\n",
    "\n",
    "train = ImageFolder(current_path + '/animal_pics/training_set', transform = TRANSFORM)\n",
    "test = ImageFolder(current_path + '/animal_pics/test_set', transform = TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained net\n",
    "net = models.vgg16(pretrained=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting last layer to have 2 output features\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting params to update for the optimizer\n",
    "params_to_update = []\n",
    "update_params_name = ['classifier.6.weight','classifier.6.bias']\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name in update_params_name :\n",
    "        param.requires_grad = True\n",
    "        params_to_update.append(param)\n",
    "    else :\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, epochs, train_loader, test_loader):\n",
    "\n",
    "    stats_dict = {'all_loss':[], 'mean_loss':[], 'accuracy':[], 'models':[]}\n",
    "\n",
    "    try : \n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            print(f'EPOCH {epoch+1}')\n",
    "            print()\n",
    "            \n",
    "            batch_counter = 0\n",
    "            mean_loss_for_epoch = []\n",
    "\n",
    "            for x,y in train_loader :\n",
    "\n",
    "                if batch_counter % 5 == 0 :\n",
    "                    print(str(batch_counter) + ' --> ', end = '', flush = True)\n",
    "\n",
    "                model.train()\n",
    "                opt.zero_grad()\n",
    "\n",
    "                yhat = model.forward(x)\n",
    "                loss = criterion(yhat,y)\n",
    "\n",
    "                stats_dict['all_loss'].append(loss.item())\n",
    "                mean_loss_for_epoch.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                batch_counter += 1\n",
    "\n",
    "            stats_dict['mean_loss'].append(mean(mean_loss_for_epoch))\n",
    "            stats_dict['models'].append(model)\n",
    "            print()\n",
    "            print('Mean loss for epoch :', mean(mean_loss_for_epoch))\n",
    "\n",
    "            correct = 0\n",
    "            incorrect = 0\n",
    "            for x,y in test_loader :\n",
    "\n",
    "                model.eval()\n",
    "                yhat = model.forward(x)\n",
    "\n",
    "                for i in range(len(yhat)) : \n",
    "                    if torch.argmax(yhat[i]) == y[i] :\n",
    "                        correct += 1\n",
    "                    else :\n",
    "                        incorrect += 1\n",
    "\n",
    "            acc = (correct / (incorrect + correct)) * 100\n",
    "            print('Accuracy on test set : ', acc, '%')\n",
    "            stats_dict['accuracy'].append(acc)\n",
    "            print('-' * 50)\n",
    "\n",
    "        return stats_dict\n",
    "    except :\n",
    "        return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH 1\n",
      "\n",
      "0 --> 5 --> 10 --> 15 --> 20 --> 25 --> 30 --> 35 --> 40 --> 45 --> 50 --> 55 --> 60 --> 65 --> 70 --> 75 --> 80 --> 85 --> 90 --> 95 --> 100 --> 105 --> 110 --> 115 --> 120 --> 125 --> 130 --> 135 --> 140 --> 145 --> 150 --> 155 --> 160 --> 165 --> 170 --> 175 --> 180 --> 185 --> 190 --> 195 --> 200 --> 205 --> 210 --> 215 --> 220 --> 225 --> 230 --> 235 --> 240 --> 245 --> 250 --> 255 --> 260 --> 265 --> 270 --> 275 --> 280 --> 285 --> 290 --> 295 --> 300 --> 305 --> 310 --> 315 --> 320 --> 325 --> 330 --> 335 --> 340 --> 345 --> 350 --> 355 --> 360 --> 365 --> 370 --> 375 --> 380 --> 385 --> 390 --> 395 --> 400 --> 405 --> 410 --> 415 --> 420 --> 425 --> 430 --> 435 --> 440 --> 445 --> 450 --> 455 --> 460 --> 465 --> 470 --> 475 --> 480 --> 485 --> 490 --> 495 --> 500 --> 505 --> 510 --> 515 --> 520 --> 525 --> 530 --> 535 --> 540 --> 545 --> 550 --> 555 --> 560 --> 565 --> 570 --> 575 --> 580 --> 585 --> 590 --> 595 --> 600 --> 605 --> 610 --> 615 --> 620 --> 625 --> 630 --> 635 --> 640 --> 645 --> 650 --> 655 --> 660 --> 665 --> 670 --> 675 --> 680 --> 685 --> 690 --> 695 --> 700 --> 705 --> 710 --> 715 --> 720 --> 725 --> 730 --> 735 --> 740 --> 745 --> 750 --> 755 --> 760 --> 765 --> 770 --> 775 --> 780 --> 785 --> 790 --> 795 --> \n",
      "Mean loss for epoch : 0.3691142326090811\n",
      "Accuracy on test set :  93.2 %\n",
      "--------------------------------------------------\n",
      "EPOCH 2\n",
      "\n",
      "0 --> 5 --> 10 --> 15 --> 20 --> 25 --> "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "train_loader = DataLoader(dataset = train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test, batch_size = 2000, shuffle = True)\n",
    "\n",
    "opt = optim.SGD(params_to_update, lr = LR, momentum = MOMENTUM)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "stats = train_model(model = net, opt = opt, criterion = loss, epochs = EPOCHS, train_loader = train_loader, test_loader = test_loader)"
   ]
  }
 ]
}