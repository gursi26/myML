{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"WGAN-GP.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1QWktU7G4RwS","executionInfo":{"status":"ok","timestamp":1612686718345,"user_tz":-480,"elapsed":4438,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["import torch \n","from torch import nn,optim\n","import torchvision \n","from torchvision import datasets,transforms\n","from torch.utils.data import DataLoader \n","from torch.utils.tensorboard import SummaryWriter "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3uI_JuH--Fa","executionInfo":{"status":"ok","timestamp":1612686745924,"user_tz":-480,"elapsed":31989,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}},"outputId":"e96aa19d-8413-40f4-cded-27609b0dcef7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aTz34SLOHPpX","executionInfo":{"status":"ok","timestamp":1612686745926,"user_tz":-480,"elapsed":31988,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["### Discriminator has no fc layers\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, channels_img, features_d):\n","        super(Discriminator,self).__init__()\n","\n","        # Input shape : N x channels_img x 64 x 64\n","        self.disc = nn.Sequential(\n","            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), # 32x32\n","            nn.LeakyReLU(0.2),\n","\n","            self._block(features_d, features_d*2, 4, 2, 1), # 16x16\n","            self._block(features_d*2, features_d*4, 4, 2, 1), # 8x8\n","            self._block(features_d*4, features_d*8, 4, 2, 1), # 4x4\n","\n","            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0) # 1x1\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","    def forward(self,x):\n","        return self.disc(x)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5HiqM5vHPpY","executionInfo":{"status":"ok","timestamp":1612686745927,"user_tz":-480,"elapsed":31987,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["class Generator(nn.Module):\n","\n","    def __init__(self, z_dim, channels_img, features_g):\n","        super(Generator,self).__init__()\n","\n","        self.gen = nn.Sequential(\n","\n","            self._block(z_dim, features_g*16, 4, 1, 0), # 4x4\n","            self._block(features_g*16, features_g*8, 4, 2, 1), # 8x8\n","            self._block(features_g*8, features_g*4, 4, 2, 1), # 16x16\n","            self._block(features_g*4, features_g*2, 4, 2, 1), # 32x32\n","\n","            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1), # 64x64\n","            nn.Tanh()\n","\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self,x):\n","        return self.gen(x)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1Ds7-HTxxha","executionInfo":{"status":"ok","timestamp":1612690437429,"user_tz":-480,"elapsed":1032,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["def gradient_penalty(critic, real, fake, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * alpha + fake * (1 - alpha)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"dy3HDvVSHPpY","executionInfo":{"status":"ok","timestamp":1612686745929,"user_tz":-480,"elapsed":31984,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["def initialize_weights(model):\n","    for m in model.modules() :\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsrhkCjgHPpZ","executionInfo":{"status":"ok","timestamp":1612686745929,"user_tz":-480,"elapsed":31982,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["def test():\n","    N, in_channels, H, W = 8, 3, 64, 64\n","    z_dim = 100 \n","    x = torch.randn((N, in_channels, H, W))\n","\n","    disc = Discriminator(in_channels, 8)\n","    initialize_weights(disc)\n","    assert disc(x).shape == (N, 1, 1, 1)\n","\n","    gen = Generator(z_dim, in_channels, 8)\n","    initialize_weights(gen)\n","\n","    z = torch.randn((N, z_dim, 1, 1))\n","    assert gen(z).shape == (N, in_channels, H, W)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3T99L9O4RwY","executionInfo":{"status":"ok","timestamp":1612686745930,"user_tz":-480,"elapsed":31981,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["BATCH_SIZE = 64\n","LR = 1e-4\n","CHANNELS_IMG = 3\n","FEATURES_D = 64\n","Z_DIM = 100\n","FEATURES_G = 64\n","IMAGE_SIZE = 64\n","step = 0\n","CRITIC_ITERATIONS = 5\n","LAMBDA_GP = 10"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLRk-wH4HPpZ","executionInfo":{"status":"ok","timestamp":1612686783724,"user_tz":-480,"elapsed":69749,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}},"outputId":"36fdfdf4-1372-462f-8458-57ca3a9888fd"},"source":["dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","t = transforms.Compose([\n","    transforms.Resize(IMAGE_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)])\n","])\n","\n","#data = datasets.MNIST('./data', transform=t, download=True, train=True)\n","data = datasets.ImageFolder('/content/drive/MyDrive/dataset', transform=t)\n","loader = DataLoader(dataset=data, batch_size=BATCH_SIZE, shuffle=True)\n","print(data)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Dataset ImageFolder\n","    Number of datapoints: 5909\n","    Root location: /content/drive/MyDrive/dataset\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=64, interpolation=PIL.Image.BILINEAR)\n","               ToTensor()\n","               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","           )\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M9JYY7qkHPpa","executionInfo":{"status":"ok","timestamp":1612686793919,"user_tz":-480,"elapsed":79943,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["disc = Discriminator(CHANNELS_IMG, FEATURES_D)\n","#disc.load_state_dict(torch.load('/content/drive/MyDrive/WGAN/disc123.pt', map_location=dev))\n","disc.to(dev)\n","initialize_weights(disc)\n","opt_disc = optim.Adam(disc.parameters(), lr = LR, betas = (0.0,0.9))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"pprbAXTaHPpa","executionInfo":{"status":"ok","timestamp":1612686793920,"user_tz":-480,"elapsed":79942,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}}},"source":["gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_G)\n","#gen.load_state_dict(torch.load('/content/drive/MyDrive/WGAN/gen123.pt', map_location=dev))\n","gen.to(dev)\n","initialize_weights(gen)\n","opt_gen = optim.Adam(gen.parameters(), lr = LR, betas = (0.0,0.9))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7BlOXbOHPpa","executionInfo":{"status":"ok","timestamp":1612686795246,"user_tz":-480,"elapsed":81254,"user":{"displayName":"Gursimar Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdBe-S1v5LBK1_4iNi_aPNxPrsjTAC92b6U0UdnA=s64","userId":"04214674680415061255"}},"outputId":"d5f5b04e-36a7-4920-8934-d39443b53415"},"source":["fixed_noise = torch.randn((32, Z_DIM, 1, 1)).to(dev)\n","\n","writer_real = SummaryWriter(log_dir='runs/real')\n","writer_fake = SummaryWriter(log_dir='runs/fake')\n","\n","gen.train()\n","disc.train()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discriminator(\n","  (disc): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sW4i-FBS5WMY","outputId":"92bcec88-497b-4f3d-ccd5-3a12f45273ae"},"source":["EPOCHS = 200\n","for epoch in range(EPOCHS):\n","    for batch_idx, (real, _) in enumerate(loader):\n","        if batch_idx == 92 : \n","          break\n","        real = real.to(dev)\n","\n","        for _ in range(CRITIC_ITERATIONS):\n","            noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(dev)\n","            fake = gen(noise)\n","\n","            disc_real = disc(real).reshape(-1)\n","            disc_fake = disc(fake).reshape(-1)\n","\n","            gp = gradient_penalty(disc, real, fake, device = dev)\n","\n","            loss_D = (-(torch.mean(disc_real) - torch.mean(disc_fake)) + LAMBDA_GP * gp)\n","\n","            disc.zero_grad()\n","            loss_D.backward(retain_graph=True)\n","            opt_disc.step()\n","\n","        # train gen\n","        output = disc(fake).reshape(-1)\n","        loss_G = -torch.mean(output)\n","        gen.zero_grad()\n","        loss_G.backward()\n","        opt_gen.step()\n","\n","        if batch_idx % 92 == 0:\n","            print(\n","                f\"Epoch [{epoch}/{EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n","                  Loss D: {loss_D:.4f}, loss G: {loss_G:.4f}\"\n","            )\n","\n","            with torch.no_grad():\n","                fake = gen(fixed_noise)\n","                # take out (up to) 32 examples\n","                img_grid_real = torchvision.utils.make_grid(\n","                    real[:32], normalize=True\n","                )\n","                img_grid_fake = torchvision.utils.make_grid(\n","                    fake[:32], normalize=True\n","                )\n","\n","                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n","                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n","\n","            step += 1\n","\n","    torch.save(disc.state_dict(), f'/content/drive/MyDrive/WGAN_GP/disc{step}.pt')\n","    torch.save(gen.state_dict(), f'/content/drive/MyDrive/WGAN_GP/gen{step}.pt')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [0/200] Batch 0/93                   Loss D: -16.4660, loss G: 97.9360\n","Epoch [1/200] Batch 0/93                   Loss D: -15.9266, loss G: 92.6589\n","Epoch [2/200] Batch 0/93                   Loss D: -16.6279, loss G: 87.3288\n","Epoch [3/200] Batch 0/93                   Loss D: -16.9854, loss G: 92.9295\n","Epoch [4/200] Batch 0/93                   Loss D: -17.5651, loss G: 90.6648\n","Epoch [5/200] Batch 0/93                   Loss D: -13.7457, loss G: 89.0549\n","Epoch [6/200] Batch 0/93                   Loss D: -13.1712, loss G: 87.7857\n","Epoch [7/200] Batch 0/93                   Loss D: -12.5203, loss G: 91.0849\n","Epoch [8/200] Batch 0/93                   Loss D: -14.5989, loss G: 84.5986\n","Epoch [9/200] Batch 0/93                   Loss D: -13.7905, loss G: 88.8603\n","Epoch [10/200] Batch 0/93                   Loss D: -15.3539, loss G: 84.3146\n","Epoch [11/200] Batch 0/93                   Loss D: -11.8278, loss G: 95.0411\n","Epoch [12/200] Batch 0/93                   Loss D: -13.5490, loss G: 84.8170\n","Epoch [13/200] Batch 0/93                   Loss D: -12.5155, loss G: 95.2750\n","Epoch [14/200] Batch 0/93                   Loss D: -13.6638, loss G: 91.4282\n","Epoch [15/200] Batch 0/93                   Loss D: -14.4737, loss G: 88.1122\n","Epoch [16/200] Batch 0/93                   Loss D: -11.6782, loss G: 88.9716\n","Epoch [17/200] Batch 0/93                   Loss D: -12.5599, loss G: 93.9761\n","Epoch [18/200] Batch 0/93                   Loss D: -11.3692, loss G: 91.5154\n","Epoch [19/200] Batch 0/93                   Loss D: -13.6978, loss G: 92.5785\n","Epoch [20/200] Batch 0/93                   Loss D: -16.2429, loss G: 92.7024\n","Epoch [21/200] Batch 0/93                   Loss D: -13.5463, loss G: 100.5970\n","Epoch [22/200] Batch 0/93                   Loss D: -11.7537, loss G: 93.8521\n","Epoch [23/200] Batch 0/93                   Loss D: -14.2855, loss G: 103.9100\n","Epoch [24/200] Batch 0/93                   Loss D: -14.0923, loss G: 97.6450\n","Epoch [25/200] Batch 0/93                   Loss D: -13.9863, loss G: 102.1349\n","Epoch [26/200] Batch 0/93                   Loss D: -13.1294, loss G: 107.8944\n","Epoch [27/200] Batch 0/93                   Loss D: -14.0209, loss G: 100.6233\n","Epoch [28/200] Batch 0/93                   Loss D: -12.6923, loss G: 104.3749\n","Epoch [29/200] Batch 0/93                   Loss D: -15.4414, loss G: 100.5700\n","Epoch [30/200] Batch 0/93                   Loss D: -15.4857, loss G: 106.9547\n","Epoch [31/200] Batch 0/93                   Loss D: -12.9306, loss G: 105.0436\n","Epoch [32/200] Batch 0/93                   Loss D: -12.3720, loss G: 111.0114\n","Epoch [33/200] Batch 0/93                   Loss D: -12.1006, loss G: 103.7407\n","Epoch [34/200] Batch 0/93                   Loss D: -14.5621, loss G: 105.0670\n","Epoch [35/200] Batch 0/93                   Loss D: -15.5639, loss G: 104.2466\n","Epoch [36/200] Batch 0/93                   Loss D: -13.8265, loss G: 109.7282\n","Epoch [37/200] Batch 0/93                   Loss D: -12.4467, loss G: 112.8342\n","Epoch [38/200] Batch 0/93                   Loss D: -14.0592, loss G: 121.5507\n","Epoch [39/200] Batch 0/93                   Loss D: -12.7742, loss G: 109.6814\n","Epoch [40/200] Batch 0/93                   Loss D: -14.2934, loss G: 108.1488\n","Epoch [41/200] Batch 0/93                   Loss D: -16.4467, loss G: 108.3666\n","Epoch [42/200] Batch 0/93                   Loss D: -11.5229, loss G: 118.3058\n","Epoch [43/200] Batch 0/93                   Loss D: -12.5714, loss G: 120.4435\n","Epoch [44/200] Batch 0/93                   Loss D: -12.0973, loss G: 118.7930\n","Epoch [45/200] Batch 0/93                   Loss D: -13.7478, loss G: 129.6333\n","Epoch [46/200] Batch 0/93                   Loss D: -12.1816, loss G: 124.0562\n","Epoch [47/200] Batch 0/93                   Loss D: -12.9601, loss G: 121.4214\n","Epoch [48/200] Batch 0/93                   Loss D: -12.2502, loss G: 123.5877\n","Epoch [49/200] Batch 0/93                   Loss D: -13.9225, loss G: 121.1462\n","Epoch [50/200] Batch 0/93                   Loss D: -10.9536, loss G: 129.3311\n","Epoch [51/200] Batch 0/93                   Loss D: -13.2786, loss G: 122.0990\n","Epoch [52/200] Batch 0/93                   Loss D: -10.4562, loss G: 119.5052\n","Epoch [53/200] Batch 0/93                   Loss D: -13.2889, loss G: 127.9618\n","Epoch [54/200] Batch 0/93                   Loss D: -16.2041, loss G: 136.4688\n","Epoch [55/200] Batch 0/93                   Loss D: -13.8370, loss G: 127.9345\n","Epoch [56/200] Batch 0/93                   Loss D: -14.3377, loss G: 131.0093\n","Epoch [57/200] Batch 0/93                   Loss D: -12.9680, loss G: 136.3222\n","Epoch [58/200] Batch 0/93                   Loss D: -16.0217, loss G: 143.6263\n","Epoch [59/200] Batch 0/93                   Loss D: -11.5704, loss G: 140.3702\n","Epoch [60/200] Batch 0/93                   Loss D: -11.8360, loss G: 146.7005\n","Epoch [61/200] Batch 0/93                   Loss D: -14.5450, loss G: 145.1057\n","Epoch [62/200] Batch 0/93                   Loss D: -16.0164, loss G: 145.9599\n","Epoch [63/200] Batch 0/93                   Loss D: -11.0375, loss G: 141.8125\n","Epoch [64/200] Batch 0/93                   Loss D: -17.7954, loss G: 146.4857\n","Epoch [65/200] Batch 0/93                   Loss D: -11.0543, loss G: 150.0850\n","Epoch [66/200] Batch 0/93                   Loss D: -13.2390, loss G: 142.1188\n","Epoch [67/200] Batch 0/93                   Loss D: -15.8205, loss G: 152.1566\n","Epoch [68/200] Batch 0/93                   Loss D: -13.9809, loss G: 152.2274\n","Epoch [69/200] Batch 0/93                   Loss D: -13.6797, loss G: 148.6157\n","Epoch [70/200] Batch 0/93                   Loss D: -11.5303, loss G: 150.4382\n","Epoch [71/200] Batch 0/93                   Loss D: -14.6233, loss G: 156.0346\n","Epoch [72/200] Batch 0/93                   Loss D: -15.0768, loss G: 152.2392\n","Epoch [73/200] Batch 0/93                   Loss D: -13.7023, loss G: 159.8075\n","Epoch [74/200] Batch 0/93                   Loss D: -18.1926, loss G: 155.0485\n","Epoch [75/200] Batch 0/93                   Loss D: -14.7871, loss G: 166.0635\n","Epoch [76/200] Batch 0/93                   Loss D: -12.8266, loss G: 156.0826\n","Epoch [77/200] Batch 0/93                   Loss D: -13.7390, loss G: 159.3886\n","Epoch [78/200] Batch 0/93                   Loss D: -13.9453, loss G: 166.3979\n","Epoch [79/200] Batch 0/93                   Loss D: -13.2867, loss G: 163.4124\n","Epoch [80/200] Batch 0/93                   Loss D: -13.6635, loss G: 162.8853\n","Epoch [81/200] Batch 0/93                   Loss D: -12.9848, loss G: 167.8971\n","Epoch [82/200] Batch 0/93                   Loss D: -16.0208, loss G: 172.6515\n","Epoch [83/200] Batch 0/93                   Loss D: -13.7131, loss G: 172.8078\n","Epoch [84/200] Batch 0/93                   Loss D: -14.1025, loss G: 179.4434\n","Epoch [85/200] Batch 0/93                   Loss D: -12.1189, loss G: 172.9294\n","Epoch [86/200] Batch 0/93                   Loss D: -14.6533, loss G: 174.5965\n","Epoch [87/200] Batch 0/93                   Loss D: -13.3114, loss G: 177.8219\n","Epoch [88/200] Batch 0/93                   Loss D: -15.8940, loss G: 173.9424\n","Epoch [89/200] Batch 0/93                   Loss D: -17.0695, loss G: 186.3333\n","Epoch [90/200] Batch 0/93                   Loss D: -13.3556, loss G: 177.9762\n","Epoch [91/200] Batch 0/93                   Loss D: -15.4796, loss G: 169.4484\n","Epoch [92/200] Batch 0/93                   Loss D: -18.9081, loss G: 181.6616\n","Epoch [93/200] Batch 0/93                   Loss D: -19.0616, loss G: 178.4517\n","Epoch [94/200] Batch 0/93                   Loss D: -15.0898, loss G: 177.4517\n","Epoch [95/200] Batch 0/93                   Loss D: -15.8075, loss G: 180.2701\n","Epoch [96/200] Batch 0/93                   Loss D: -16.4539, loss G: 180.6628\n","Epoch [97/200] Batch 0/93                   Loss D: -17.7757, loss G: 188.5024\n","Epoch [98/200] Batch 0/93                   Loss D: -14.1742, loss G: 186.9884\n","Epoch [99/200] Batch 0/93                   Loss D: -20.2308, loss G: 192.3491\n","Epoch [100/200] Batch 0/93                   Loss D: -17.2447, loss G: 195.5327\n","Epoch [101/200] Batch 0/93                   Loss D: -17.2190, loss G: 194.0270\n","Epoch [102/200] Batch 0/93                   Loss D: -16.3531, loss G: 196.3187\n","Epoch [103/200] Batch 0/93                   Loss D: -16.1773, loss G: 192.2482\n","Epoch [104/200] Batch 0/93                   Loss D: -16.1941, loss G: 190.2340\n","Epoch [105/200] Batch 0/93                   Loss D: -11.1366, loss G: 199.1075\n","Epoch [106/200] Batch 0/93                   Loss D: -10.7353, loss G: 196.3969\n","Epoch [107/200] Batch 0/93                   Loss D: -16.8889, loss G: 199.5667\n","Epoch [108/200] Batch 0/93                   Loss D: -17.5414, loss G: 194.7105\n","Epoch [109/200] Batch 0/93                   Loss D: -14.2202, loss G: 200.7862\n","Epoch [110/200] Batch 0/93                   Loss D: -15.6011, loss G: 204.1664\n","Epoch [111/200] Batch 0/93                   Loss D: -12.9554, loss G: 201.3818\n","Epoch [112/200] Batch 0/93                   Loss D: -12.8198, loss G: 197.7363\n","Epoch [113/200] Batch 0/93                   Loss D: -14.3649, loss G: 199.9901\n","Epoch [114/200] Batch 0/93                   Loss D: -19.0716, loss G: 199.9204\n","Epoch [115/200] Batch 0/93                   Loss D: -13.3167, loss G: 206.1371\n","Epoch [116/200] Batch 0/93                   Loss D: -15.2268, loss G: 198.9430\n","Epoch [117/200] Batch 0/93                   Loss D: -16.2588, loss G: 209.1307\n","Epoch [118/200] Batch 0/93                   Loss D: -16.4399, loss G: 201.5611\n","Epoch [119/200] Batch 0/93                   Loss D: -17.8933, loss G: 205.1594\n","Epoch [120/200] Batch 0/93                   Loss D: -20.0892, loss G: 202.0793\n","Epoch [121/200] Batch 0/93                   Loss D: -17.6126, loss G: 201.1852\n","Epoch [122/200] Batch 0/93                   Loss D: -14.0086, loss G: 213.1686\n","Epoch [123/200] Batch 0/93                   Loss D: -13.7291, loss G: 209.9435\n","Epoch [124/200] Batch 0/93                   Loss D: -14.6455, loss G: 205.0531\n","Epoch [125/200] Batch 0/93                   Loss D: -17.2432, loss G: 204.3607\n","Epoch [126/200] Batch 0/93                   Loss D: -18.8826, loss G: 212.6502\n","Epoch [127/200] Batch 0/93                   Loss D: -15.1438, loss G: 218.1513\n","Epoch [128/200] Batch 0/93                   Loss D: -19.1196, loss G: 215.1507\n","Epoch [129/200] Batch 0/93                   Loss D: -15.9569, loss G: 212.3412\n","Epoch [130/200] Batch 0/93                   Loss D: -17.0676, loss G: 207.2390\n","Epoch [131/200] Batch 0/93                   Loss D: -19.6607, loss G: 208.0984\n","Epoch [132/200] Batch 0/93                   Loss D: -16.9726, loss G: 216.3414\n","Epoch [133/200] Batch 0/93                   Loss D: -16.5197, loss G: 219.8252\n","Epoch [134/200] Batch 0/93                   Loss D: -22.0197, loss G: 211.9807\n","Epoch [135/200] Batch 0/93                   Loss D: -15.4409, loss G: 209.1200\n","Epoch [136/200] Batch 0/93                   Loss D: -16.9167, loss G: 217.5103\n","Epoch [137/200] Batch 0/93                   Loss D: -18.0634, loss G: 215.6830\n","Epoch [138/200] Batch 0/93                   Loss D: -16.3047, loss G: 209.1749\n","Epoch [139/200] Batch 0/93                   Loss D: -17.8512, loss G: 219.4850\n","Epoch [140/200] Batch 0/93                   Loss D: -15.3654, loss G: 222.0821\n","Epoch [141/200] Batch 0/93                   Loss D: -15.5590, loss G: 214.6777\n","Epoch [142/200] Batch 0/93                   Loss D: -15.9763, loss G: 216.0940\n","Epoch [143/200] Batch 0/93                   Loss D: -13.6484, loss G: 217.1792\n","Epoch [144/200] Batch 0/93                   Loss D: -17.5628, loss G: 218.7136\n","Epoch [145/200] Batch 0/93                   Loss D: -18.0193, loss G: 222.9031\n","Epoch [146/200] Batch 0/93                   Loss D: -16.2335, loss G: 223.4423\n","Epoch [147/200] Batch 0/93                   Loss D: -18.7715, loss G: 223.3174\n","Epoch [148/200] Batch 0/93                   Loss D: -17.2768, loss G: 224.5791\n","Epoch [149/200] Batch 0/93                   Loss D: -18.7727, loss G: 225.5118\n","Epoch [150/200] Batch 0/93                   Loss D: -14.3169, loss G: 229.2376\n","Epoch [151/200] Batch 0/93                   Loss D: -19.3261, loss G: 226.7129\n","Epoch [152/200] Batch 0/93                   Loss D: -22.9660, loss G: 231.6862\n","Epoch [153/200] Batch 0/93                   Loss D: -16.7112, loss G: 232.2309\n","Epoch [154/200] Batch 0/93                   Loss D: -13.8974, loss G: 227.0148\n","Epoch [155/200] Batch 0/93                   Loss D: -15.5973, loss G: 236.0443\n","Epoch [156/200] Batch 0/93                   Loss D: -14.6737, loss G: 226.8208\n","Epoch [157/200] Batch 0/93                   Loss D: -16.7596, loss G: 231.7513\n","Epoch [158/200] Batch 0/93                   Loss D: -14.7602, loss G: 229.0548\n","Epoch [159/200] Batch 0/93                   Loss D: -14.9541, loss G: 227.9286\n","Epoch [160/200] Batch 0/93                   Loss D: -20.4911, loss G: 231.1973\n","Epoch [161/200] Batch 0/93                   Loss D: -14.3392, loss G: 228.9167\n","Epoch [162/200] Batch 0/93                   Loss D: -14.8187, loss G: 236.4522\n","Epoch [163/200] Batch 0/93                   Loss D: -15.9913, loss G: 231.5791\n","Epoch [164/200] Batch 0/93                   Loss D: -17.9078, loss G: 230.6146\n","Epoch [165/200] Batch 0/93                   Loss D: -21.5765, loss G: 233.1763\n","Epoch [166/200] Batch 0/93                   Loss D: -18.2576, loss G: 234.2273\n","Epoch [167/200] Batch 0/93                   Loss D: -17.4383, loss G: 233.3745\n","Epoch [168/200] Batch 0/93                   Loss D: -16.8168, loss G: 239.6293\n","Epoch [169/200] Batch 0/93                   Loss D: -16.1225, loss G: 231.8199\n","Epoch [170/200] Batch 0/93                   Loss D: -17.3345, loss G: 240.0352\n","Epoch [171/200] Batch 0/93                   Loss D: -19.4764, loss G: 232.9976\n","Epoch [172/200] Batch 0/93                   Loss D: -16.8715, loss G: 240.6185\n","Epoch [173/200] Batch 0/93                   Loss D: -17.3117, loss G: 239.4444\n","Epoch [174/200] Batch 0/93                   Loss D: -16.2607, loss G: 240.3808\n","Epoch [175/200] Batch 0/93                   Loss D: -15.3250, loss G: 232.6233\n","Epoch [176/200] Batch 0/93                   Loss D: -28.0856, loss G: 231.3020\n","Epoch [177/200] Batch 0/93                   Loss D: -14.0925, loss G: 238.4347\n","Epoch [178/200] Batch 0/93                   Loss D: -19.5233, loss G: 238.6115\n","Epoch [179/200] Batch 0/93                   Loss D: -17.0268, loss G: 236.0426\n","Epoch [180/200] Batch 0/93                   Loss D: -18.4526, loss G: 232.4776\n","Epoch [181/200] Batch 0/93                   Loss D: -16.9130, loss G: 236.5158\n","Epoch [182/200] Batch 0/93                   Loss D: -16.5127, loss G: 236.6268\n","Epoch [183/200] Batch 0/93                   Loss D: -17.8890, loss G: 234.0475\n","Epoch [184/200] Batch 0/93                   Loss D: -19.6845, loss G: 244.7253\n"],"name":"stdout"}]}]}